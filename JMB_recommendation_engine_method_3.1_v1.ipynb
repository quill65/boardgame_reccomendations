{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Board game recommendation engine\n",
    "### Example of user rating predictor for Matt Borthwick's rating challenge\n",
    "#### Using Single Value Decomposition, and Alternating Least Squares\n",
    "\n",
    "#### John Burt\n",
    "\n",
    "\n",
    "#### Purpose of this notebook:\n",
    "\n",
    "This is an example of how to implement a user rating predictor, using the boardgamegeek dataset. The method reads user data and the the provided test data with missing ratings, creates a ratings predictor based on the user data, generates ratings predictions for the test data and saves the results to a csv file that can be presented to Matt's website for evaluation.\n",
    "\n",
    "Method transforms the user ratings data via Single Value Decomposition (SVD) into a 4D \"user-space\", where users with similar ratings are closer in the space. For a given test set of userID and gameID, the predictor:\n",
    "- Finds the user-space coordinate of the user.\n",
    "- Searches for nearby users who have rated the game.\n",
    "- Returns the mean rating of a specified number of neighbors who have rated the game.\n",
    "\n",
    "Run with the all users data, this method gets an RMSE score of 1.233\n",
    "\n",
    "**Note about Alternating Least Squares:** Prior to the SVD transform to user-space, the raw ratings data is converted to a 2D array of userID x gameID, with cells containing either ratings, or NaNs for games that a user hasn't rated (which is most of the array). SVD wants a fully filled (dense) array, so the unrated cells with NaNs need to be filled with something useful. There are several ways to do this, including just setting the NaN cells to 0, or using an imputer to fill the empty cells with mean game ratings. Using those methods had poor results, so I tried another method called Alternating Least Squares (ALS), which improves the accuracy by quite a bit. The downside to ALS is that it is very memory and processor intensive: you cannot run ALS on the full dataset due to memory limitations, and even if you restrict the training set to only users in the test set, as I do here, it can take a long time for the ALS to finish. So, fair warning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the notebook and load the data\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "# load the boardgame user data\n",
    "testdata = pd.read_csv('boardgame-users-test.csv') \n",
    "#userdata = pd.read_csv('boardgame-elite-users.csv')\n",
    "#userdata = pd.read_csv('boardgame-frequent-users.csv')\n",
    "userdata = pd.read_csv('boardgame-users.csv') \n",
    "\n",
    "# rename the userID column\n",
    "userdata=userdata.rename(columns = {\"Compiled from boardgamegeek.com by Matt Borthwick\":'userID'})\n",
    "\n",
    "# load the boardgame title data\n",
    "titledata = pd.read_csv('boardgame-titles.csv')\n",
    "\n",
    "# rename the gameID column\n",
    "titledata=titledata.rename(columns = {\"boardgamegeek.com game ID\":'gameID'})\n",
    "\n",
    "# for titledata set game ID as the index\n",
    "titledata = titledata.set_index(\"gameID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Alternating Least Squares algorithm. I found this at the source below. It seems popular as a method of inferring missing ratings to fill ratings matrices prior to feeding them into PCA or SVD analyses. For more on this method, see the link below.\n",
    "\n",
    "I got the idea to use this and adapted the code from:\n",
    "https://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/\n",
    "\n",
    "### WARNING: this function is computationally heavy. It can take a while to complete and if the dataset is too large, the code will barf out with a memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(Q, X, Y, W):\n",
    "    return np.sum((W * (Q - np.dot(X, Y)))**2)\n",
    "\n",
    "def do_ALS(Q, n_iterations=10, lambda_=0.1, n_factors=100, weighted=True, verbose=True ):\n",
    "    \n",
    "    W = Q>0.5\n",
    "    W[W == True] = 1\n",
    "    W[W == False] = 0\n",
    "    # To be consistent with our Q matrix\n",
    "    W = W.astype(np.float64, copy=False)\n",
    "\n",
    "    m, n = Q.shape\n",
    "\n",
    "    X = 5 * np.random.rand(m, n_factors) \n",
    "    Y = 5 * np.random.rand(n_factors, n)\n",
    "    \n",
    "    if verbose: print(\"\\n\")\n",
    "\n",
    "    if not weighted:\n",
    "        errors = []\n",
    "        for ii in range(n_iterations):\n",
    "            X = np.linalg.solve(np.dot(Y, Y.T) + lambda_ * np.eye(n_factors), \n",
    "                                np.dot(Y, Q.T)).T\n",
    "            Y = np.linalg.solve(np.dot(X.T, X) + lambda_ * np.eye(n_factors),\n",
    "                                np.dot(X.T, Q))\n",
    "            if ii % 2 == 0:\n",
    "                if verbose: print(\"iteration %d, error = %4.2f\"%(ii,get_error(Q, X, Y, W)))\n",
    "\n",
    "            errors.append(get_error(Q, X, Y, W))\n",
    "        Q_hat = np.dot(X, Y)\n",
    "        if verbose: print(\"Error of rated games: %4.2f\"%(get_error(Q, X, Y, W)))\n",
    "\n",
    "    else:\n",
    "        errors = []\n",
    "        for ii in range(n_iterations):\n",
    "            for u, Wu in enumerate(W):\n",
    "                X[u] = np.linalg.solve(np.dot(Y, np.dot(np.diag(Wu), Y.T)) + lambda_ * np.eye(n_factors),\n",
    "                                       np.dot(Y, np.dot(np.diag(Wu), Q[u].T))).T\n",
    "            for i, Wi in enumerate(W.T):\n",
    "                Y[:,i] = np.linalg.solve(np.dot(X.T, np.dot(np.diag(Wi), X)) + lambda_ * np.eye(n_factors),\n",
    "                                         np.dot(X.T, np.dot(np.diag(Wi), Q[:, i])))\n",
    "            if ii % 2 == 0:\n",
    "                if verbose: print(\"iteration %d, error = %4.2f\"%(ii,get_error(Q, X, Y, W)))\n",
    "            errors.append(get_error(Q, X, Y, W))\n",
    "\n",
    "        if verbose: print(\"iteration %d, error = %4.2f\"%(ii,get_error(Q, X, Y, W)))\n",
    "        Q_hat = np.dot(X,Y)\n",
    "\n",
    "    return Q_hat, X, Y, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given test set of userID and gameID, search user-space for nearby users who have rated the game. Return a mean rating for a specified number of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def estimate_ratings(testdata, coords, train_p, numneighbors2use):\n",
    "    \n",
    "    # iterate through all test pairs of userID, gameID\n",
    "    rating = []\n",
    "    for index, rec in testdata.iterrows():\n",
    "        \n",
    "        # index of test user in training data\n",
    "        userindex = train_p.index.get_loc(rec.userID) \n",
    "\n",
    "        # coordinate in user-space of test user\n",
    "        targetcoord = coords[userindex,:]\n",
    "\n",
    "        # get euclidean distances of all points to targetcoord    \n",
    "        dists = cdist(np.reshape(targetcoord,(1,-1)),coords) \n",
    "\n",
    "        # sort by distance\n",
    "        ind, = np.argsort(dists)\n",
    "\n",
    "        # Search neighbors for ratings for the specified game.\n",
    "        # Start with nearest neighbor and collect specified number of \n",
    "        #  nearby ratings.\n",
    "        ratings = []\n",
    "        for user in train_p.index[ind]:\n",
    "            if ~np.isnan(train_p[rec.gameID][user]):\n",
    "                ratings.append(train_p[rec.gameID][user])\n",
    "                if len(ratings) >= numneighbors2use:\n",
    "                    break\n",
    "\n",
    "        # get mean rating of collected user ratings\n",
    "        rating.append(np.mean(ratings))\n",
    "\n",
    "    return np.array(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration 0, error = 1599950.24\n",
      "iteration 2, error = 1322745.33\n",
      "iteration 4, error = 1256091.93\n",
      "iteration 6, error = 1236436.40\n",
      "iteration 7, error = 1231704.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, SVD, SparsePCA, KernelPCA, TruncatedSVD, MiniBatchSparsePCA\n",
    "from sklearn.preprocessing import normalize, StandardScaler, Normalizer\n",
    "\n",
    "# only train with users in the test data set\n",
    "traindata = userdata[userdata.userID.isin(set(testdata.userID))]\n",
    "\n",
    "# only test with users that exist in both training and test set\n",
    "testdata_filt = testdata[testdata.userID.isin(set(userdata.userID))]\n",
    "\n",
    "# pivot the training data to get roaw=userID, cols=gameID, cells=ratings (or NaNs if no rating)\n",
    "train_p = traindata.pivot(index=\"userID\", columns=\"gameID\", values=\"rating\")\n",
    "\n",
    "# run ALS on pivit data to fill in NaN cells with a useful ratings estimate\n",
    "lambda_ = 0.1 # note: changing this doesn't seem to affect much\n",
    "n_factors = 10 # smaller #factors seems to give better results\n",
    "n_iterations = 8 # 8-10 iterations works best for all-user data, 15-20 for elite & frequent users data\n",
    "# train_p_filled is our filled in matrix, errors lets us plot how things went\n",
    "train_p_filled, X, Y, errors = do_ALS(train_p.fillna(0).values, n_iterations=n_iterations, \n",
    "                             lambda_=lambda_, n_factors=n_factors, weighted=True )\n",
    "\n",
    "# convert numpy array output back to pandas dataframe\n",
    "train_p_filled = pd.DataFrame(train_p_filled,index=train_p.index,columns=train_p.columns)\n",
    "\n",
    "# threshold ratings to between 1-10\n",
    "train_p_filled = train_p_filled.clip_upper(10)\n",
    "train_p_filled = train_p_filled.clip_lower(1)\n",
    "\n",
    "# transform filled pivot array into 4D \"user-space\" coordinates\n",
    "numdims =  4\n",
    "coords = TruncatedSVD(n_components=numdims).fit_transform(train_p_filled)\n",
    "\n",
    "# get estimated ratings for test set\n",
    "numneighbors2use = 50 # 50 users seems to work best\n",
    "ratings_est = np.array(estimate_ratings(testdata_filt, coords, train_p_filled, numneighbors2use))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate submission file for Matt's website.\n",
    " \n",
    " Submit the file here: \n",
    " http://dive-into.info/5848/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 2\n",
    "initials = \"JMB\"\n",
    "test_pred = testdata_filt.copy()\n",
    "test_pred[\"rating\"] = ratings_est\n",
    "test_pred = test_pred.set_index([\"userID\"])\n",
    "test_pred.to_csv(initials+\"_predicted-v\"+str(version)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
